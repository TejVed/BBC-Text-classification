{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"path = '/kaggle/input'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pylab inline\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom pprint import pprint\nfrom xgboost import XGBClassifier\nfrom gensim.models import Phrases, LdaModel\nfrom gensim.corpora import Dictionary\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import brown\nfrom nltk import FreqDist\nfrom wordcloud import WordCloud \nfrom collections import OrderedDict\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a Df with 5 columns: Directory, Category, FileName, Title, Text\ncategories = []\ntitles = []\nall_data = []\n#df = pd.DataFrame()\n# dirname\nfor dirname, categoryname, filenames in os.walk(path):\n    # filename\n    for filename in filenames:\n        if filename == 'README.TXT':\n            filenames.remove(filename)\n        else:\n            # Absolute path\n            current_file = os.path.abspath(os.path.join(dirname, filename))\n            open_file = open(current_file, 'r', encoding=\"latin-1\")\n            # text_data\n            text_data = open_file.read().split('\\n')\n            text_data = list(filter(None, text_data))\n            titles.append(text_data[0])\n            all_data.append((dirname, dirname.rsplit('/',1)[1], filename, text_data[0], text_data[1:]))\n            #data_df = f\"Directory: {dirname}, Category: {dirname.rsplit('/',1)[1]}, FileName: {filename}, Title: {text_data[0]}, Text: {text_data[1:]}\"\n            #print(data_df)\ndf = pd.DataFrame(all_data, columns=['directory', 'category', 'fileName', 'title', 'text'])\ndf['text'] = df.text.astype(str)\nprint(df.head())  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.category.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bar_plot=df.category.value_counts().plot(kind='barh', figsize=(8, 6), color='teal')\nplt.xlabel(\"Nr. of Artciles\", labelpad=14)\nplt.ylabel(\"Category\", labelpad=14)\nplt.title(\"Nr. of Articles in category\", y=1.02, color='navy')\n\nfor index, value in enumerate(df.category.value_counts()):\n    plt.text(value, index, str(value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text[1][:1000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Label Encoder","metadata":{}},{"cell_type":"code","source":"# 0 - business, 1 -entertainment, 2 - politics, 3 - sport, 4 - tech\nlabel_enc = LabelEncoder()\ndf['label'] = label_enc.fit_transform(df['category'])\ndf.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# An array of words\ndf_txt = np.array(df['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-process and vectorize text","metadata":{}},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\n\ndef docs_preprocessor(docs):\n    # Remain only letters\n    tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n    \n    for idx in range(len(docs)):\n         # Convert to lowercase\n        docs[idx] = docs[idx].lower() \n        # Split into words\n        docs[idx] = tokenizer.tokenize(docs[idx])  \n    \n    # Lemmatize all words with len>2 in documents \n    lemmatizer = WordNetLemmatizer()\n    docs = [[nltk.stem.WordNetLemmatizer().lemmatize(token) for token in doc if len(token) > 2 and token not in stopwords] for doc in docs]\n    #stemmer = SnowballStemmer('english')\n    #docs = [[stemmer.stem(token) for token in doc if len(token) > 2 and token not in stopwords] for doc in docs]\n         \n    return docs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_txt = docs_preprocessor(df_txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add bigrams and trigrams to docs (only ones that appear 10 times or more)\nbigram = Phrases(df_txt, min_count=10)\ntrigram = Phrases(bigram[df_txt])\n\nfor idx in range(len(df_txt)):\n    for token in bigram[df_txt[idx]]:\n        if '_' in token:\n            df_txt[idx].append(token)\n    for token in trigram[df_txt[idx]]:\n        if '_' in token:\n            df_txt[idx].append(token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove rare words:","metadata":{}},{"cell_type":"code","source":"# Create a dictionary representation of the documents\ndictionary = Dictionary(df_txt)\nprint('Nr. of unique words in initital documents:', len(dictionary))\n\n# Filter out words that occur less than 10 documents, or more than 20% of the documents\ndictionary.filter_extremes(no_below=10, no_above=0.2)\nprint('Nr. of unique words after removing rare and common words:', len(dictionary))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text2'] = df_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text3'] = [' '.join(map(str, j)) for j in df['text2']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[1475:1480,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Word Vectors","metadata":{}},{"cell_type":"code","source":"#vectorizer = TfidfVectorizer(stop_words = 'english', lowercase=True)\nvectorizer = TfidfVectorizer(input='content', analyzer = 'word', lowercase=True, stop_words='english',\\\n                                   ngram_range=(1, 3), min_df=40, max_df=0.20,\\\n                                  norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\ntext_vector = vectorizer.fit_transform(df.text3)\ndtm = text_vector.toarray()\nfeatures = vectorizer.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = pd.DataFrame(data = text_vector.todense(), columns = vectorizer.get_feature_names())\nh.iloc[990:1000,280:300]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = [dictionary.doc2bow(txt) for txt in df_txt]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of unique tokens: {len(dictionary)}')\nprint(f'Number of documents: {len(corpus)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Frequency distribution for dictionary\n#fdist = nltk.FreqDist(dictionary)\n#fdist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Model","metadata":{}},{"cell_type":"code","source":"X = text_vector\ny = df.label.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc1 = RandomForestClassifier(random_state = 42)\nsvc1.fit(X_train, y_train)\nsvc1_pred = svc1.predict(X_test)\n#print(f\"Train Accuracy: {svc1.score(X_train, y_train)*100:.3f}%\")\nprint(f\"Test Accuracy: {svc1.score(X_test, y_test)*100:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc2 = XGBClassifier(random_state = 42, use_label_encoder=False)\nsvc2.fit(X_train, y_train)\nsvc2_pred = svc2.predict(X_test)\n#print(f\"Train Accuracy: {svc2.score(X_train, y_train)*100:.3f}%\")\nprint(f\"Test Accuracy: {svc2.score(X_test, y_test)*100:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc3 = SGDClassifier(random_state = 42)\nsvc3.fit(X_train, y_train)\nsvc3_pred = svc3.predict(X_test)\n#print(f\"Train Accuracy: {svc3.score(X_train, y_train)*100:.3f}%\")\nprint(f\"Test Accuracy: {svc3.score(X_test, y_test)*100:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc4 = KNeighborsClassifier()\n#pprint(svc4.get_params())\nsvc4.fit(X_train, y_train)\nsvc4_pred = svc4.predict(X_test)\n#print(f\"Train Accuracy: {svc4.score(X_train, y_train)*100:.3f}%\")\nprint(f\"Test Accuracy: {svc4.score(X_test, y_test)*100:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))\n\n# Takes in a model, trains the model, and evaluates the model on the test set\ndef fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(X_test)\n    model_mae = mae(y_test, model_pred)\n    \n    # Return the performance metric\n    return model_mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc1_mae = fit_and_evaluate(svc1)\nsvc2_mae = fit_and_evaluate(svc2)\nsvc3_mae = fit_and_evaluate(svc3)\nsvc4_mae = fit_and_evaluate(svc4)\n#print(svc1_mae, svc2_mae, svc3_mae, svc4_mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nfig = plt.figure(figsize(8, 6))\n\n# Dataframe to hold the results\nmodel_comparison = pd.DataFrame({'model': ['RandomForest Classifier', 'XGBClassifier', \n                                           'SGDClassifier', 'KNeighborsClassifier'\n                                          ],\n                                 'mae': [svc1_mae, svc2_mae, \n                                         svc3_mae, svc4_mae]})\n\n# Horizontal bar chart of test mae\nmodel_comparison.sort_values('mae', ascending = False).plot(x = 'model', y = 'mae', kind = 'barh',\n                                                           color = 'yellow', edgecolor = 'black')\n\n# Plot formatting\nplt.ylabel('')\nplt.yticks(size = 14)\nplt.xlabel('Mean Absolute Error')\nplt.xticks(size = 14)\nplt.title('Model Comparison on Test MAE', size = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Cloud","metadata":{}},{"cell_type":"code","source":"wc = WordCloud(width = 800, height = 800, \n                background_color ='white', \n               stopwords=stopwords,\n                min_font_size = 10, random_state=42).generate(df.text3.to_string())\n\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wc)\nplt.tight_layout(pad = 0) \nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in df.category.unique():\n    wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords=stopwords,\n                   min_font_size = 10, random_state=42)\n    wc.generate(df.text3[(df.category == x)].to_string())\n    \n    plt.imshow(wc)\n    plt.tight_layout(pad = 0) \n    plt.title(x)\n    plt.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clustering","metadata":{}},{"cell_type":"code","source":"# Set training parameters\nnum_topics = 5\nchunksize = 500 # Number of documents to consider at once (affects the memory consumption)\npasses = 20 # Number of passes through documents\niterations = 400\neval_every = 1  \n\n# Make a index to word dictionary\ntemp = dictionary[0] \nid2word = dictionary.id2token\n\nmodel = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', random_state=78, \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Frequency of top words in each topic\ndef explore_topic(lda_model, topic_number, topn, output=True):\n    terms = []\n    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n        terms += [term]\n        if output:\n            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))   \n    return terms\n\ntopic_summaries = []\n\nprint(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\nfor i in range(num_topics):\n    print('Topic '+str(i)+'\\n')\n    tmp = explore_topic(model,topic_number=i, topn=10, output=True)\n    topic_summaries += [tmp[:5]]\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign a human-interpretable \ntop_labels = {0: 'business', 1:'sport', 2:'tech', 3:'entertainment', 4:'politics'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_dist =[]\nfor d in corpus:\n    tmp = {i:0 for i in range(num_topics)}\n    tmp.update(dict(model[d]))\n    vals = list(OrderedDict(tmp).values())\n    top_dist += [array(vals)]\n\ndef get_doc_topic_dist(model, corpus, kwords=False):\n    '''\n    LDA transformation, for each doc only returns topics with non-zero weight\n    This function makes a matrix transformation of docs in the topic space.\n    \n    model: the LDA model\n    corpus: the documents\n    kwords: if True adds and returns the keys\n    '''\n    top_dist =[]\n    keys = []\n\n    for d in corpus:\n        tmp = {i:0 for i in range(num_topics)}\n        tmp.update(dict(model[d]))\n        vals = list(OrderedDict(tmp).values())\n        top_dist += [array(vals)]\n        if kwords:\n            keys += [array(vals).argmax()]\n\n    return array(top_dist), keys\n\ntop_dist, lda_keys= get_doc_topic_dist(model, corpus, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ws = []\nfor n in range(len(dtm)):\n    inds = int0(argsort(dtm[n])[::-1][:4])\n    tmp = [features[i] for i in inds]\n    \n    top_ws += [' '.join(tmp)]\n    \ndf['Text_Rep'] = pd.DataFrame(top_ws)\ndf['clusters'] = pd.DataFrame(lda_keys)\ndf['clusters'].fillna(10, inplace=True)\n\ncluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue'}\n\ndf['colors'] = df['clusters'].apply(lambda j: cluster_colors[j])\n# Assign a human-interpretable labels \ndf['category_lda'] = df['clusters'].replace([0, 1, 2, 3, 4],['business','sport','tech','entertainment','politics'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clusters visualization","metadata":{}},{"cell_type":"code","source":"from bokeh.plotting import figure, show, output_notebook, save\nfrom bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(top_dist)\n\ndf['X_tsne'] =X_tsne[:, 0]\ndf['Y_tsne'] =X_tsne[:, 1]\n\noutput_notebook()\n\nsource = ColumnDataSource(dict(\n    x=df['X_tsne'],\n    y=df['Y_tsne'],\n    color=df['colors'],\n    label=df['clusters'].apply(lambda l: top_labels[l]),\n    topic_key= df['clusters'],\n    title= df[u'title'],\n    content = df['text3'],\n    legend_field=df['category_lda']\n))\n\ndf = df.drop(columns=['colors','Text_Rep','X_tsne','Y_tsne'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Topics visualization'\n\nplot_lda = figure(plot_width=1000, plot_height=600,\n                     title=title, \n                     x_axis_type=None, y_axis_type=None, min_border=1)\nplot_lda.scatter(x='x', y='y', legend_field='legend_field',  source=source,\n                 color='color', alpha=0.6, size=5.0)\n\n# hover tools\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips = {\"content\": \"Title: @title, KeyWords: @content - Topic: @topic_key \"}\nplot_lda.legend.location = \"top_left\"\n\nshow(plot_lda)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Correct categories: {len(df[df.category==df.category_lda])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Percentage of correct categories: {round(len(df[df.category==df.category_lda])/len(df)*100,2)}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}